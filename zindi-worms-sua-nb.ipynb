{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10616332,"sourceType":"datasetVersion","datasetId":6258558}],"dockerImageVersionId":30804,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold, StratifiedKFold, TimeSeriesSplit, GroupKFold\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.svm import SVR, LinearSVR\nimport lightgbm as lgb\nfrom tqdm import tqdm\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# --- 1. Define Column Lists and Location Blacklist ---\ncat_cols = ['Category_Health_Facility_UUID', 'Disease']\nkey_cols = ['Transformed_Latitude', 'Transformed_Longitude', 'Year', 'Month']\nloc_cols = ['Transformed_Latitude', 'Transformed_Longitude']\nblacklist_locations = [\n    'ID_02f25962-4419-4e72-9402-3f6c513ec454',\n    'ID_1b04544b-5c96-4053-9a26-a1ab26f8a4a2',\n    'ID_361566e9-3fa1-4c0e-9778-84d5dc605feb',\n    'ID_57abdcf0-63df-4ea8-ace6-816787f63911',\n    'ID_ecc21577-6c70-4ba3-a2a2-1d9c0e0477f0'\n]\nunique_locations = [\n    'ID_00cd8292-dd85-4fa3-8148-9592e88a1651','ID_02b8390e-f332-4ed5-9e24-9163f09ba478','ID_02f25962-4419-4e72-9402-3f6c513ec454','ID_030ee103-7194-452c-b0e6-58142de19bf6','ID_0358ea0e-af2d-450e-8124-4144ca7860fa','ID_069981aa-3e3d-4008-8460-0b9a564b93ce','ID_0bb8622b-8f9f-4443-b63e-5d1cc9cc46dd','ID_13386124-f7bf-404c-a905-4960e15f1b42','ID_1881eac3-60eb-4a11-b124-3f435335bda0','ID_1883a4d5-4803-4d4b-8c36-af9a1ada280b','ID_193775ae-c108-4564-917e-b99ce259dddf','ID_1b04544b-5c96-4053-9a26-a1ab26f8a4a2','ID_1e0e8bde-3d9e-4846-8ad8-1fa3787d66e6','ID_283f5ebc-25e8-45c0-b20b-f063d851dc2f','ID_2cdf865f-07a9-428f-a5c5-fa19cdf03a49','ID_309b6f8a-c625-4efd-891a-c2d654984998','ID_361566e9-3fa1-4c0e-9778-84d5dc605feb','ID_3a11929e-3317-476d-99f7-1bd9fb58f018','ID_3b04a772-80cb-421b-a64a-01d098fa7e46','ID_41ee755f-c8fb-4605-998d-5dc3d005167c','ID_41fbe2d6-4f32-4976-9048-94252493b5a9','ID_42d6c301-12c2-487e-8955-a126843210be','ID_4705dcf9-f034-4584-b8ab-c8d855f57f94','ID_4716e9d7-2b2b-4d04-adff-e11f43993707','ID_47f5bc9e-f219-451c-b2f7-27e83c58cf7b','ID_57abdcf0-63df-4ea8-ace6-816787f63911','ID_679ca97a-9b9d-400b-9809-b04fc6b4a1c1','ID_69a65631-3c68-4800-9b1a-32996da92c7a','ID_6aa33452-3f11-4c71-819d-e15e6b11f949','ID_6bf0aeb9-67be-4f3a-ad4d-2ac5b7f9c497','ID_6f399bd7-f181-432a-bddc-9fff8dca7f81','ID_704a38c1-35ca-4e81-ab81-02fcf41d1f72','ID_776a9566-8bcb-4936-83a9-e8ef57834a9f','ID_777b4bae-d725-4a1a-9f4a-31b42ebc3e10','ID_79e64e06-dccb-401a-b74d-5d8e4d863615','ID_8016b072-00e9-47d3-b26b-0bc55f541664','ID_82613626-d68d-4d21-b6f0-f90be35004d7','ID_83880e91-c9db-47f3-8cbb-2a1b485260eb','ID_880270f1-8d75-4bc9-965d-27c5a22ec0eb','ID_8b36c0ac-b46c-4c9a-af37-d6717faa340e','ID_8c25ad79-9b7f-4ba0-8e03-69cbfcd39569','ID_8efc0ee1-e183-4518-afc4-83b63b79b1f8','ID_8f26ab15-3d25-43ed-bd52-8b2431c6ba90','ID_90ae3848-6fe9-4326-95ee-d7b8679e7494','ID_90d75acb-4528-43d5-b688-2853541772ea','ID_989f8bc0-6d2a-4ad0-9b8b-641540895ddf','ID_9a9f0ca2-bff2-4e4a-bb38-d9c83ef0602c','ID_a81e0fdf-34f9-4334-b25d-20194fad0381','ID_ac8a77f4-353d-4351-82ed-29cf82cc7775','ID_bd1b57a9-066c-497a-bdef-7ae8e849db14','ID_ddf39ebf-663b-4f32-92b1-a700598dd4b9','ID_e5b10b72-c677-430e-8eee-289c77eeac0b','ID_ebec04b4-99d8-462d-983e-cfbe2294a90e','ID_ecc21577-6c70-4ba3-a2a2-1d9c0e0477f0','ID_ef027061-3d99-4215-8487-374b4ab4699a','ID_f2c528af-f0d7-4c88-ba81-257166928227','ID_f8aa9c00-4ac5-4cea-a386-de165a3671ca','ID_feed50e1-074b-47c9-8ecd-0f65ac9305e6'\n]\ntrain_cols = ['Month', 'Year', 'Total_1', 'Total_2',\n                 'Transformed_Latitude', 'Transformed_Longitude'] + cat_cols\ntoilets_cols = ['10u', '10v', '2d', '2t', 'evabs', 'evaow', 'evatc', 'evavt', 'albedo', 'lshf', 'lai_hv', 'lai_lv', 'pev', 'ro', 'src',\n                 'skt', 'es', 'stl1', 'stl2', 'stl3', 'stl4', 'ssro', 'slhf', 'ssr', 'str', 'sp', 'sro', 'sshf', 'ssrd', 'strd', 'e', 'tp', 'swvl1', 'swvl2', 'swvl3', 'swvl4']\ndisease_cols = ['Diarrhea', 'Dysentery', 'Intestinal Worms', 'Malaria', 'Typhoid', 'Cholera']\ndis_others = ['Dysentery', 'Dysentery', 'Typhoid', 'Cholera']\nwaste_cols = [f'{c}_wm' for c in toilets_cols] # Corrected waste_cols suffix to '_wm'\nwater_cols = [f'{c}_water' for c in toilets_cols]\ngrp_cols = ['Disease', 'Location', 'Category_Health_Facility_UUID']\ntarget = 'Total'\ncorr_features = ['10u_water', '10v_water', '2d_water', '2t_water', 'evabs_water', 'evaow_water', 'evatc_water', 'evavt_water', 'albedo_water', 'lshf_water', 'lai_hv_water', 'lai_lv_water', 'pev_water', 'ro_water', 'src_water', 'skt_water', 'es_water', 'stl1_water', 'stl2_water', 'stl3_water', 'stl4_water', 'ssro_water', 'slhf_water', 'ssr_water', 'str_water', 'sp_water', 'sro_water', 'sshf_water', 'ssrd_water', 'strd_water', 'e_water', 'tp_water', 'swvl1_water', 'swvl2_water', 'swvl3_water', 'swvl4_water'] # Example - replace with actual correlated features\n\n\n# --- 2. Load Datasets ---\ntrain = pd.read_csv(\"/kaggle/input/sua-outsmarting-outbreaks-challenge/outsmarting-outbreaks-challenge20241207-28044-iehqcg/Train.csv\")\ntest = pd.read_csv(\"/kaggle/input/sua-outsmarting-outbreaks-challenge/outsmarting-outbreaks-challenge20241207-28044-iehqcg/Test.csv\")\ntoilets = pd.read_csv(\"/kaggle/input/sua-outsmarting-outbreaks-challenge/outsmarting-outbreaks-challenge20241207-28044-iehqcg/toilets.csv\")\nwaste_management = pd.read_csv(\"/kaggle/input/sua-outsmarting-outbreaks-challenge/outsmarting-outbreaks-challenge20241207-28044-iehqcg/waste_management.csv\")\nwater_sources = pd.read_csv(\"/kaggle/input/sua-outsmarting-outbreaks-challenge/outsmarting-outbreaks-challenge20241207-28044-iehqcg/water_sources.csv\")\n\ntest[target] = 0\ntest['Predicted_Total'] = np.nan\n\n# --- 3. Aggregate Training Data ---\ntrain_sum = train.groupby(['Disease', 'Location', 'Year', 'Month', 'Category_Health_Facility_UUID'] + loc_cols)[target].sum().reset_index()\n\n# --- 4. Feature Engineering and Preprocessing Function ---\ndef feature_engineering_and_preprocessing(df):\n    df['day'] = 1\n    df['date'] = df[['Year', 'Month', 'day']].astype(str).apply(' '.join, axis=1)\n    df['date'] = pd.to_datetime(df['date'])\n    df['tag'] = 1\n    df['tag_id'] = df.groupby(['Disease', 'Location', 'Year', 'Month', 'Category_Health_Facility_UUID'])['tag'].cumsum()\n    df['time_index'] = df['date'].astype(int)\n    df = df.sort_values(by=['date'])\n    df['start_date'] = df['Location'].map(df.groupby(['Location'])['date'].first().to_dict())\n    df['diff_date'] = (df['date'] - df['start_date']).dt.days\n    return df\n\ntrain_sum = feature_engineering_and_preprocessing(train_sum)\ntrain = feature_engineering_and_preprocessing(train)\ntest = feature_engineering_and_preprocessing(test)\ntest_sum = feature_engineering_and_preprocessing(test)\n\n# --- 5. Lag Feature Engineering Function ---\ndef create_lag_features(df, data, lag_months=36):\n    date_col = data[['date']]\n    temp_df = data[[target, 'Disease', 'date', 'Location', 'tag_id']]\n    for i in range(1, lag_months + 1):\n        temp_df['date'] = date_col['date'] + pd.DateOffset(months=i)\n        df = df.merge(temp_df, on=['Disease', 'date', 'Location', 'tag_id'], how='left', suffixes=('', f'_{i}'))\n    return df\n\ntrain = train.sort_values(by=['Year', 'Month'])\n\ndata_for_lag = train.groupby(['Disease','date','Location','tag_id'])[target].mean().reset_index()\ndata_sum_for_lag = train_sum.groupby(['Disease','date','Location','tag_id'])[target].mean().reset_index()\n\ntrain = create_lag_features(train, data_for_lag)\ntest = create_lag_features(test, data_for_lag)\ntrain_sum = create_lag_features(train_sum, data_sum_for_lag)\ntest_sum = create_lag_features(test_sum, data_sum_for_lag)\n\n# --- 6. Merge Environmental Data ---\ntoilets[loc_cols] = toilets[loc_cols].astype(float).round(0)\nwaste_management[loc_cols] = waste_management[loc_cols].astype(float).round(0)\nwater_sources[loc_cols] = water_sources[loc_cols].astype(float).round(0)\n\ntoilets = toilets.groupby(key_cols)[toilets_cols].mean().reset_index()\nwaste_management = waste_management.groupby(key_cols)[toilets_cols].sum().reset_index()\nwater_sources = water_sources.groupby(key_cols)[toilets_cols].mean().reset_index()\n\ndef merge_environmental_data(df):\n    df['hosp_id'] = df[['Disease', 'Location']].astype(str).apply('_'.join, axis=1)\n    df[loc_cols] = df[loc_cols].astype(float).round(0)\n    env_dfs = [(toilets, '_toilets'), (waste_management, '_wm'), (water_sources, '_water')] # Corrected suffix here\n    for env_df, suffix in env_dfs:\n        df = df.merge(env_df, on=key_cols, how='left', suffixes=('', suffix))\n    return df\n\ntrain = merge_environmental_data(train)\ntrain_sum = merge_environmental_data(train_sum)\ntest = merge_environmental_data(test)\ntest_sum = merge_environmental_data(test_sum)\n\n# --- 7. Function to Generate Static Features ---\ndef get_static_features(df, cols):\n    df['medianT'] = df[cols].median(axis=1)\n    df['meanT'] = df[cols].mean(axis=1)\n    df['maxT'] = df[cols].max(axis=1)\n    df['sumT'] = df[cols].sum(axis=1)\n    df['prodT'] = df[cols].prod(axis=1)\n    df['skewT'] = df[cols].skew(axis=1)\n    df['kurtT'] = df[cols].kurt(axis=1)\n    df['semT'] = df[cols].sem(axis=1)\n    df['stdT'] = df[cols].std(axis=1)\n    df['q2T'] = df[cols].quantile(0.75, axis=1)\n    df['q4T'] = df[cols].quantile(0.95, axis=1)\n    df['q5T'] = df[cols].quantile(0.99, axis=1)\n    df['zeros_countT'] = (df[cols] != 0).sum(axis=1)\n    return df\n\n\n# --- 8. Feature Correlation Analysis ---\ncorrelation = train_sum[train_sum['Year'] < 2023][water_cols + waste_cols + toilets_cols + [target]].corr()[target].sort_values(ascending=False)\ncorrelation = abs(correlation)\ncorr_features = correlation[correlation > 0.05].index[1:].tolist()\nprint(f\"Features selected based on correlation: {corr_features}\")\n\n\n# --- 9. Centralized Cross-Validation Function ---\ndef generic_train_cv(X, y, valid_df, test_df, model, feature_cols, cv_type='kfold', n_splits=3, shuffle=True, rs=42, use_scaler=False):\n    if cv_type == 'kfold':\n        cv = KFold(n_splits=n_splits, shuffle=shuffle, random_state=rs)\n    elif cv_type == 'timeseries':\n        cv = TimeSeriesSplit(n_splits=n_splits, test_size=50)\n    else:\n        raise ValueError(f\"Invalid cv_type: {cv_type}. Must be 'kfold' or 'timeseries'.\")\n\n    valid_predictions = []; test_predictions = []\n    X_cols_filled = X[feature_cols].fillna(0)\n\n    for fold, (train_index, val_index) in enumerate(cv.split(X, y)):\n        X_train, X_val = X_cols_filled.iloc[train_index], X_cols_filled.iloc[val_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[val_index]\n\n        if use_scaler:\n            scaler_x = RobustScaler()\n            X_train = scaler_x.fit_transform(X_train)\n            X_val = scaler_x.transform(X_val)\n            valid_df[feature_cols] = scaler_x.transform(valid_df[feature_cols])\n            test_df[feature_cols] = scaler_x.transform(test_df[feature_cols])\n            scaler_y = RobustScaler()\n            y_train = scaler_y.fit_transform(y_train.to_frame())\n\n        model_fold = model\n        model_fold.fit(X_train, y_train)\n\n        y_hat_valid = model_fold.predict(valid_df[feature_cols].fillna(0))\n        y_hat_test = model_fold.predict(test_df[feature_cols].fillna(0))\n\n        if use_scaler:\n            y_hat_valid = scaler_y.inverse_transform(y_hat_valid.reshape(1, -1)).flatten()\n            y_hat_test = scaler_y.inverse_transform(y_hat_test.reshape(1, -1)).flatten()\n\n        valid_predictions.append(y_hat_valid)\n        test_predictions.append(y_hat_test)\n\n    return np.mean(valid_predictions, axis=0), np.mean(test_predictions, axis=0)\n\n\n# --- 10. Centralized Model Training Function ---\ndef generic_model_trainer(df_train, df_test, disease_name, model_params, train_params):\n    year_valid = train_params.get('year_valid', 2022)\n    train_year = train_params.get('train_year', 2023)\n    use_diff_target = train_params.get('use_diff_target', False)\n    use_org_target = train_params.get('use_org_target', False)\n    use_ratio_target = train_params.get('use_ratio_target', False)\n    use_median_target = train_params.get('use_median_target', True)\n    par_dec = train_params.get('par_dec', 1)\n    na_val = train_params.get('na_val', 9)\n    use_scaler = train_params.get('use_scaler', False)\n    cv_type = train_params.get('cv_type', 'kfold')\n    cv_splits = train_params.get('cv_splits', 4)\n    cv_shuffle = train_params.get('cv_shuffle', True)\n    cv_rs = train_params.get('cv_rs', 42)\n    model_type = train_params.get('model_type', 'lgbm')\n\n\n    print(f\"\\nTraining model for {disease_name}\")\n\n    valid_preds_all = pd.DataFrame() # DataFrame to store validation predictions\n\n    for i in tqdm(df_test['date'].unique()):\n        temp_test = df_test[(df_test['date'] == i)]\n        temp_train = df_train\n        month = temp_test['date'].dt.month.values[0]\n\n        static_feature_cols = [f'Total_{j}' for j in range(month, month + 12)]\n        temp_train = get_static_features(temp_train, static_feature_cols)\n        temp_test = get_static_features(temp_test, static_feature_cols)\n        X_valid_month = temp_train[(temp_train['Year'] == year_valid) & (temp_train['Month'] == month)].copy() # Copy to avoid SettingWithCopyWarning\n        y_valid_month = temp_train[(temp_train['Year'] == year_valid) & (temp_train['Month'] == month)][target].copy() # Copy to avoid SettingWithCopyWarning\n        lag_target_median = 'medianT' # Define lag_target_median here\n\n        X_valid_month['valid_index'] = X_valid_month.index # Keep index for merging back predictions\n\n\n        if use_org_target:\n            feature_cols_org = static_feature_cols + corr_features + ['Year', 'Month'] + [c for c in temp_train.columns if c.endswith('T') and c not in ['Category_Health_Facility_UUID', 'ID']]\n            feature_cols_lr_org = ['medianT']\n\n            train_data_org = temp_train[(temp_train['Year'] < train_year)].fillna(na_val)\n            X_org = train_data_org.drop(target, axis=1)\n            y_org = train_data_org[target]\n            X_org = X_org[y_org.notna()]\n            y_org = y_org[X_org.index]\n\n            model_lgbm_org = lgb.LGBMRegressor(**model_params.get('lgbm_org', lgb_params_general)) # Get params or default\n            model_lr_org = LinearSVR(max_iter=400, random_state=432) # Use LinearSVR directly\n\n            y_hat_valid_lgbm_org, y_hat_test_lgbm_org = generic_train_cv(X_org, y_org, X_valid_month, temp_test, model_lgbm_org, feature_cols_org, cv_type=cv_type, n_splits=cv_splits, shuffle=cv_shuffle, rs=cv_rs)\n            y_hat_valid_lr_org, y_hat_test_lr_org = generic_train_cv(X_org, y_org, X_valid_month, temp_test, model_lr_org, feature_cols_lr_org, cv_type=cv_type, n_splits=cv_splits, shuffle=cv_shuffle, rs=cv_rs, use_scaler=use_scaler)\n\n            y_hat_valid_org = (y_hat_valid_lgbm_org * 0.5) + (y_hat_valid_lr_org * 0.5)\n            y_hat_test_org = (y_hat_test_lgbm_org * 0.5) + (y_hat_test_lr_org * 0.5)\n\n            valid_preds_month_org = pd.DataFrame({'pred_'+disease_name.lower()+'_org': y_hat_valid_org, 'valid_index': X_valid_month['valid_index']}) # Store valid preds with index\n            valid_preds_all = pd.concat([valid_preds_all, valid_preds_month_org]) # Append to all valid preds\n\n            df_test.loc[(df_test['date'] == i), 'Predicted_Total_'+disease_name.lower()+'_org'] = y_hat_test_org\n\n\n        if use_diff_target:\n            feature_cols_diff = corr_features + ['Month'] + [c for c in temp_train.columns if c.endswith('T') and c not in ['Category_Health_Facility_UUID', 'ID']] + [f'Total_{j}' for j in range(month, month + 6)]\n            feature_cols_lr_diff = ['medianT']\n\n\n            train_data_diff = temp_train[temp_train['Year'] < train_year][feature_cols_diff + [target, lag_target_median]].fillna(na_val)\n            train_data_diff = train_data_diff.reset_index(drop=True) # Added reset_index here\n            X_diff = train_data_diff.drop(target, axis=1)\n            y_diff = train_data_diff[target] - X_diff[lag_target_median]\n            X_diff = X_diff[y_diff.notna()]\n            y_diff = y_diff[X_diff.index]\n\n            model_lgbm_diff = lgb.LGBMRegressor(**model_params.get('lgbm_diff', lgb_params_diff)) # Get params or default\n            model_lr_diff = LinearSVR(max_iter=400, random_state=432)\n\n            y_hat_valid_lgbm_diff, y_hat_test_lgbm_diff = generic_train_cv(X_diff, y_diff, X_valid_month, temp_test, model_lgbm_diff, feature_cols_diff, cv_type=cv_type, n_splits=cv_splits, shuffle=cv_shuffle, rs=cv_rs)\n            y_hat_valid_lr_diff, y_hat_test_lr_diff = generic_train_cv(X_diff, y_diff, X_valid_month, temp_test, model_lr_diff, feature_cols_lr_diff, cv_type=cv_type, n_splits=cv_splits, shuffle=cv_shuffle, rs=cv_rs, use_scaler=use_scaler)\n\n\n            y_hat_valid_diff = (y_hat_valid_lr_diff * 0.5) + (y_hat_valid_lgbm_diff * 0.5)\n            y_hat_test_diff = (y_hat_test_lr_diff * 0.5) + (y_hat_test_lgbm_diff * 0.5)\n\n            y_hat_valid_diff = y_hat_valid_diff + X_valid_month[lag_target_median].fillna(0)\n            y_hat_test_diff = y_hat_test_diff + temp_test[lag_target_median].fillna(0)\n\n\n            valid_preds_month_diff = pd.DataFrame({'pred_'+disease_name.lower()+'_diff': y_hat_valid_diff, 'valid_index': X_valid_month['valid_index']}) # Store valid preds with index\n            valid_preds_all = pd.concat([valid_preds_all, valid_preds_month_diff]) # Append to all valid preds\n\n\n            df_test.loc[(df_test['date'] == i), 'Predicted_Total_'+disease_name.lower()+'_diff'] = y_hat_test_diff\n\n\n        if use_ratio_target:\n            feature_cols_ratio = static_feature_cols + corr_features + [c for c in temp_train.columns if c.endswith('T') and c not in ['Category_Health_Facility_UUID', 'ID']]\n            feature_cols_lr_ratio = ['medianT']\n\n\n            train_data_ratio = temp_train[temp_train['Year'] < train_year][feature_cols_ratio + [target, lag_target_median]].fillna(na_val)\n            X_ratio = train_data_ratio.drop(target, axis=1)\n            y_ratio = train_data_ratio[target] / X_ratio[lag_target_median]\n            y_ratio = y_ratio.astype(np.float32).replace([np.inf, -np.inf], np.nan)\n            X_ratio = X_ratio[y_ratio.notna()]\n            y_ratio = y_ratio[X_ratio.index]\n\n\n            model_lgbm_ratio = lgb.LGBMRegressor(**model_params.get('lgbm_ratio', lgb_params_ratio)) # Get params or default\n            model_lr_ratio = LinearSVR(max_iter=400, random_state=432)\n\n\n            y_hat_valid_lgbm_ratio, y_hat_test_lgbm_ratio = generic_train_cv(X_ratio, y_ratio, X_valid_month, temp_test, model_lgbm_ratio, feature_cols_ratio, cv_type=cv_type, n_splits=cv_splits, shuffle=cv_shuffle, rs=cv_rs)\n            y_hat_valid_lr_ratio, y_hat_test_lr_ratio = generic_train_cv(X_ratio, y_ratio, X_valid_month, temp_test, model_lr_ratio, feature_cols_lr_ratio, cv_type=cv_type, n_splits=cv_splits, shuffle=cv_shuffle, rs=cv_rs, use_scaler=use_scaler)\n\n\n            y_hat_valid_ratio = (y_hat_valid_lgbm_ratio * 0.5) + (y_hat_valid_lr_ratio * 0.5)\n            y_hat_test_ratio = (y_hat_test_lr_ratio * 0.5) + (y_hat_test_lgbm_ratio * 0.5)\n\n            y_hat_valid_ratio = y_hat_valid_ratio * X_valid_month[lag_target_median].fillna(0)\n            y_hat_test_ratio = y_hat_test_ratio * temp_test[lag_target_median].fillna(0)\n\n            valid_preds_month_ratio = pd.DataFrame({'pred_'+disease_name.lower()+'_ratio': y_hat_valid_ratio, 'valid_index': X_valid_month['valid_index']}) # Store valid preds with index\n            valid_preds_all = pd.concat([valid_preds_all, valid_preds_month_ratio]) # Append to all valid preds\n\n\n            df_train.loc[(df_train['Year'] == year_valid) & (df_train['Month'] == month), 'pred_'+disease_name.lower()+'_ratio'] = y_hat_valid_ratio\n            df_test.loc[(df_test['date'] == i), 'Predicted_Total_'+disease_name.lower()+'_ratio'] = y_hat_test_ratio\n\n\n        if use_median_target:\n            y_hat_valid_median = X_valid_month['medianT'].fillna(na_val)\n            y_hat_test_median = temp_test['medianT'].fillna(na_val)\n\n            valid_preds_month_median = pd.DataFrame({'pred_'+disease_name.lower()+'_median': y_hat_valid_median, 'valid_index': X_valid_month['valid_index']}) # Store valid preds with index\n            valid_preds_all = pd.concat([valid_preds_all, valid_preds_month_median]) # Append to all valid preds\n\n\n            df_train.loc[(df_train['Year'] == year_valid) & (df_train['Month'] == month), 'pred_'+disease_name.lower()+'_median'] = y_hat_valid_median\n            df_test.loc[(df_test['date'] == i), 'Predicted_Total_'+disease_name.lower()+'_median'] = y_hat_test_median\n\n    # Merge all validation predictions back to the training dataframe\n    valid_preds_all = valid_preds_all.merge(X_valid_month[['valid_index']], on='valid_index', how='left').set_index('valid_index')\n    df_train = df_train.merge(valid_preds_all, left_index=True, right_index=True, how='left')\n\n\n    return df_train, df_test\n\n\n# --- 11. Model and Training Parameters ---\nlgb_params_general = dict(n_estimators=40, objective='mae', verbose=-1, lambda_l1=5, max_depth=5, max_bin=100, random_state=41, learning_rate=0.05)\nlgb_params_ml = dict(n_estimators=200, objective='mae', verbose=-1, lambda_l1=5, max_depth=5, max_bin=100, random_state=41, learning_rate=0.05)\nlgb_params_diff_ml = dict(n_estimators=200, objective='mae', verbose=-1, lambda_l1=5, max_depth=5, max_bin=100, random_state=41, learning_rate=0.05)\nlgb_params_ratio_ml = dict(n_estimators=100, objective='mae', verbose=-1, lambda_l1=10, max_depth=5, max_bin=100, random_state=41, learning_rate=0.05)\nlgb_params_dr = dict(n_estimators=500, objective='mae',verbose=-1, lambda_l1=5,max_depth=5,max_bin=100, random_state=41, learning_rate=0.05)\nlgb_params_diff_dr = dict(n_estimators=1000, objective='mae',verbose=-1, lambda_l1=5,max_depth=5,max_bin=100, random_state=41, learning_rate=0.05)\nlgb_params_ratio_dr = dict(n_estimators=100, objective='mae',verbose=-1, lambda_l1=10,max_depth=5,max_bin=100, random_state=41, learning_rate=0.05)\nlgb_params_bdr = dict(n_estimators=50, objective='mae',verbose=-1, lambda_l1=5,max_depth=5,max_bin=100, random_state=41, learning_rate=0.05)\n\n\nmodel_parameter_sets = {\n    'general': {'lgbm_org': lgb_params_general},\n    'malaria': {'lgbm_org': lgb_params_ml, 'lgbm_diff': lgb_params_diff_ml},\n    'intestinal_worms': {'lgbm_org': lgb_params_ml, 'lgbm_diff': lgb_params_diff_ml, 'lgbm_ratio': lgb_params_ratio_ml},\n    'diarrhea': {'lgbm_org': lgb_params_dr, 'lgbm_diff': lgb_params_diff_dr, 'lgbm_ratio': lgb_params_ratio_dr},\n    'blacklisted_diarrhea': {'lgbm_org': lgb_params_bdr, 'lgbm_diff': lgb_params_bdr, 'lgbm_ratio': lgb_params_bdr}\n}\n\n\ntraining_parameter_sets = {\n    'dysentery_typhoid_cholera': {'year_valid': 2022, 'train_year': 2023, 'use_org_target': True}, # Example - adjust for actual needs if different\n    'malaria_original': {'year_valid': 2022, 'train_year': 2023, 'use_diff_target': True, 'use_org_target': True, 'use_median_target': True},\n    'malaria_sum': {'year_valid': 2022, 'train_year': 2023, 'use_diff_target': True, 'use_org_target': True, 'use_median_target': True},\n    'intestinal_worms': {'year_valid': 2022, 'train_year': 2023, 'use_org_target': True, 'use_diff_target': True, 'use_ratio_target': True, 'use_median_target': True, 'cv_shuffle': False, 'cv_rs': None},\n    'diarrhea_original': {'year_valid': 2022, 'train_year': 2023, 'use_diff_target': True, 'use_median_target': True, 'use_org_target': True, 'use_ratio_target': True},\n    'diarrhea_sum': {'year_valid': 2022, 'train_year': 2023, 'use_diff_target': True, 'use_median_target': True, 'use_org_target': True, 'use_ratio_target': True},\n    'blacklisted_diarrhea': {'year_valid': 2022, 'train_year': 2023, 'use_diff_target': True, 'use_org_target': True, 'use_ratio_target': True, 'use_ensemble': True, 'use_scaler': True, 'na_val': 9, 'par_dec': 1, 'cv_shuffle': False, 'cv_rs': None},\n}\n\n\n# --- 12. Train Models ---\nprint(\"\\nTraining model for ['Dysentery', 'Typhoid', 'Cholera']\")\ntemp_train_others = train[(train['Year'] >= 2021) & (train['Disease'].isin(['Dysentery', 'Typhoid', 'Cholera']))].copy() # Correct filtering\ntemp_train_others, test = generic_model_trainer(temp_train_others, test, 'Others', model_parameter_sets['general'], training_parameter_sets['dysentery_typhoid_cholera'])\n\n\nprint(\"\\nTraining model for Malaria - Original Data\")\ntemp_train_malaria = train[train['Disease'] == 'Malaria'].copy()\ntemp_train_malaria, test = generic_model_trainer(temp_train_malaria, test, 'Malaria_ml', model_parameter_sets['malaria'], training_parameter_sets['malaria_original'])\n\nprint(\"\\nTraining model for Malaria - Aggregated Data\")\ntemp_train_sum_malaria = train_sum[train_sum['Disease'] == 'Malaria'].copy()\ntemp_train_sum_malaria, test_sum = generic_model_trainer(temp_train_sum_malaria, test_sum, 'Malaria_ml_sum', model_parameter_sets['malaria'], training_parameter_sets['malaria_sum'])\n\n\nprint(\"\\nTraining model for Intestinal Worms\")\ntemp_train_iw = train[train['Disease'] == 'Intestinal Worms'].copy()\ntemp_train_iw, test = generic_model_trainer(temp_train_iw, test, 'IW', model_parameter_sets['intestinal_worms'], training_parameter_sets['intestinal_worms'])\n\n\nprint(\"\\nTraining model for Diarrhea\")\ntemp_train_dr = train[train['Disease'] == 'Diarrhea'].copy()\ntemp_train_dr, test= generic_model_trainer(temp_train_dr, test, 'Dr', model_parameter_sets['diarrhea'], training_parameter_sets['diarrhea_original'])\n\nprint(\"\\nTraining model for Diarrhea Blacklisted - Aggregated\")\ntemp_train_sum_dr = train_sum[train_sum['Disease'] == 'Diarrhea'].copy()\ntemp_train_sum_dr, test_sum= generic_model_trainer(temp_train_sum_dr, test_sum, 'Bdr', model_parameter_sets['blacklisted_diarrhea'], training_parameter_sets['blacklisted_diarrhea'])\n\n\n# --- 13. Ensemble Predictions ---\nuse_ensemble = True\nif use_ensemble: # Ensemble for Other diseases\n    ens_cols_others = ['org'] # Corrected to 'org' as per original notebook for others\n    test['Predicted_Total_ens'] = test[[f'Predicted_Total_others_{c}' for c in ens_cols_others]].mean(axis=1) # Corrected disease name\n\nif use_ensemble: # Ensemble for Malaria\n    ens_cols_malaria = ['diff', 'org']\n    test['Predicted_Total_ml_ens'] = test[[f'Predicted_Total_malaria_ml_{c}' for c in ens_cols_malaria]].mean(axis=1)\n    test_sum['Predicted_Total_ml_ens'] = test_sum[[f'Predicted_Total_malaria_ml_sum_{c}' for c in ens_cols_malaria]].mean(axis=1)\n\n    test['Predicted_Total_ml_ens_sum'] = test_sum['Predicted_Total_ml_ens'] # Corrected names\n    test['Predicted_Total_ml_diff_sum'] = test_sum['Predicted_Total_malaria_ml_sum_diff']\n    test['Predicted_Total_ml_org_sum'] = test_sum['Predicted_Total_malaria_ml_sum_org']\n\n\nif use_ensemble: # Ensemble for Intestinal Worms\n    ens_cols_iw = ['iw_org', 'iw_diff', 'iw_ratio']\n    test['Predicted_Total_iw_ens'] = test[[f'Predicted_Total_iw_{c}' for c in ens_cols_iw]].mean(axis=1)\n\n\nif use_ensemble: # Ensemble for Diarrhea\n    ens_cols_dr=['diff', 'org']\n    test['Predicted_Total_dr_ens']=test[[f'Predicted_Total_dr_{c}' for c in ens_cols_dr]].mean(axis=1)\n    test_sum['Predicted_Total_dr_ens']=test_sum[[f'Predicted_Total_dr_bdr_{c}' for c in ens_cols_dr]].mean(axis=1) # Corrected disease name\n\n    test['Predicted_Total_dr_ens_sum']=test_sum['Predicted_Total_dr_ens'] # Corrected names\n    test['Predicted_Total_dr_median_sum']=test_sum['Predicted_Total_dr_bdr_median']\n    test['Predicted_Total_dr_diff_sum']=test_sum['Predicted_Total_dr_bdr_diff']\n    test['Predicted_Total_dr_org_sum']=test_sum['Predicted_Total_dr_bdr_org']\n\n\ndef print_score(df):\n    #df['mae']=abs(df['pred']-df[target]) # This line will likely cause error as 'pred' is not defined. Consider removing or fixing.\n    df['mae_org']=abs(df['pred_others_org']-df[target]) # Corrected names\n    df['mae_diff']=abs(df['pred_others_diff']-df[target]) # Corrected names\n    df['mae_ratio']=abs(df['pred_others_ratio']-df[target]) # Corrected names\n    df['mae_median']=abs(df['pred_others_median']-df[target]) # Corrected names\n    display(df[(df['Year']==2022)].groupby(['Disease'])[['mae_org','mae_diff','mae_ratio', 'mae_median']].mean()) # Removed 'mae' as it's likely erroneous\n\n\n# --- 14. Final Prediction and Submission ---\ntest['Predicted_Total']=0\n\nblacklist = blacklist_locations # Define blacklist\n\nmalaria_cond=test['Disease'].isin(['Malaria'])\ninst_cond=test['Disease'].isin(['Intestinal Worms'])\ndr_cond=test['Disease'].isin(['Diarrhea'])\nblacklist_cond=test['Location'].isin(blacklist) & dr_cond\ndr_cond=dr_cond&~blacklist_cond\nother_cond=test['Disease'].isin(dis_others)\n\n\ntest.loc[other_cond,'Predicted_Total']=test.loc[other_cond,'Predicted_Total_ens']\ntest.loc[malaria_cond, 'Predicted_Total']=test.loc[malaria_cond, ['Predicted_Total_ml_ens_sum']].mean(axis=1)\ntest.loc[inst_cond, 'Predicted_Total']=test.loc[inst_cond, ['Predicted_Total_iw_ens']].mean(axis=1)\ntest.loc[dr_cond, 'Predicted_Total']=test.loc[dr_cond, ['Predicted_Total_dr_ens','Predicted_Total_dr_ens_sum']].mean(axis=1)\ntest.loc[blacklist_cond, 'Predicted_Total']=test.loc[blacklist_cond, ['Predicted_Total_bdr_ens_sum']].mean(axis=1)\n\n\nsub=test[['ID','Predicted_Total']]\nsub['Predicted_Total']=np.clip(sub['Predicted_Total'].fillna(0), 0,np.inf)\nsub.to_csv('test_sub.csv', index=False)\nsub['Predicted_Total'].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
